{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic\n",
    "## Ref https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Check Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "IDtest = test['PassengerId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df, n, features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list od the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\"\"\"\n",
    "    \n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        Q3 = np.percentile(df[col], 75)\n",
    "        IQR = Q3-Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list od indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1-outlier_step) | (df[col] > Q3 + outlier_step)].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)\n",
    "    \n",
    "    return multiple_outliers\n",
    "\n",
    "# detext outliers from Age, SibSp, Parch and Fare\n",
    "Outliers_to_drop = detect_outliers(train, 2, ['Age', 'SibSp', 'Parch', 'Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[Outliers_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(Outliers_to_drop, axis=0).reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat(objs = [train, test], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.fillna(np.nan)\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature analysis\n",
    "### 3.1 Numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.heatmap(train[['Survived', 'SibSp', 'Parch', 'Age', 'Fare']].corr(), annot=True, fmt='.2f', cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SibSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x='SibSp', y='Survived', data = train, kind='bar', size=6, palette='muted')\n",
    "g.despine(left=True)\n",
    "g = g.set_ylabels('survival probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x='Parch', y='Survived', data = train, kind='bar', size=6, palette='muted')\n",
    "g.despine(left=True)\n",
    "g = g.set_ylabels('survival probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(train, col='Survived')\n",
    "g = g.map(sns.distplot, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Fare'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(dataset['Fare'], label='Skewness : %.2f'%(dataset['Fare'].skew()))\n",
    "g = g.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log to Fare to reduce skewness distribution\n",
    "dataset['Fare'] = dataset['Fare'].map(lambda i: np.log(i) if i >0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(dataset['Fare'], label='Skewness : %.2f'%(dataset['Fare'].skew()))\n",
    "g = g.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.barplot(x='Sex', y='Survived', data=train)\n",
    "g = g.set_ylabel(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Sex', 'Survived']].groupby('Sex').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.barplot(x='Pclass', y='Survived', data=train)\n",
    "g = g.set_ylabel(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.barplot(x='Pclass', y='Survived', hue='Sex', data = train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Embarked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Embarked'] = dataset['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.barplot(x='Embarked', y='Survived', data=train)\n",
    "g = g.set_ylabel(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot('Pclass', col='Embarked', data=train, kind='count')\n",
    "g = g.set_ylabels(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filling missing Values\n",
    "### 4.1 Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "fig.set_size_inches(12,10)\n",
    "\n",
    "sns.boxplot(data=dataset, y='Age', x='Sex', orient = 'v', ax=axes[0][0])\n",
    "sns.boxplot(data=dataset, y='Age', x='Sex', hue='Pclass', orient = 'v', ax=axes[0][1])\n",
    "sns.boxplot(data=dataset, y='Age', x='Parch', orient = 'v', ax=axes[1][0])\n",
    "sns.boxplot(data=dataset, y='Age', x='SibSp', orient = 'v', ax=axes[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Sex'] = dataset['Sex'].map({'male':0, 'female':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.heatmap(dataset[['Age', 'Sex', 'SibSp', 'Parch', 'Pclass']].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing value of Age \n",
    "\n",
    "## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n",
    "# Index of NaN age rows\n",
    "index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_age :\n",
    "    age_med = dataset[\"Age\"].median()\n",
    "    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n",
    "    if not np.isnan(age_pred) :\n",
    "        dataset['Age'].iloc[i] = age_pred\n",
    "    else :\n",
    "        dataset['Age'].iloc[i] = age_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(x=\"Survived\", y = \"Age\",data = train, kind=\"box\")\n",
    "sns.factorplot(x=\"Survived\", y = \"Age\",data = train, kind=\"violin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature engineering\n",
    "### 5.1 Name/Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Name\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Title from Name\n",
    "dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\n",
    "dataset[\"Title\"] = pd.Series(dataset_title)\n",
    "dataset[\"Title\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Title\",data=dataset)\n",
    "plt.xticks(rotation=30, ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to categorical values Title \n",
    "dataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "dataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n",
    "dataset[\"Title\"] = dataset[\"Title\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.countplot(dataset[\"Title\"])\n",
    "g = g.set_xticklabels([\"Master\",\"Miss/Ms/Mme/Mlle/Mrs\",\"Mr\",\"Rare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\")\n",
    "g = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\n",
    "g = g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Name variable\n",
    "dataset.drop(labels = [\"Name\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Family size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a family size descriptor from SibSp and Parch\n",
    "dataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1\n",
    "sns.factorplot(x=\"Fsize\",y=\"Survived\",data = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature of family size\n",
    "dataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\n",
    "dataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\n",
    "dataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "dataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"Single\",y=\"Survived\",data=dataset,kind=\"bar\")\n",
    "g = g.set_ylabels(\"Survival Probability\")\n",
    "g = sns.factorplot(x=\"SmallF\",y=\"Survived\",data=dataset,kind=\"bar\")\n",
    "g = g.set_ylabels(\"Survival Probability\")\n",
    "g = sns.factorplot(x=\"MedF\",y=\"Survived\",data=dataset,kind=\"bar\")\n",
    "g = g.set_ylabels(\"Survival Probability\")\n",
    "g = sns.factorplot(x=\"LargeF\",y=\"Survived\",data=dataset,kind=\"bar\")\n",
    "g = g.set_ylabels(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to indicator values Title and Embarked \n",
    "dataset = pd.get_dummies(dataset, columns = [\"Title\"])\n",
    "dataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Cabin\"].head()\n",
    "dataset[\"Cabin\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Cabin\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Cabin\"][dataset[\"Cabin\"].notnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the Cabin number by the type of cabin 'X' if not\n",
    "dataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])\n",
    "dataset[\"Cabin\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(dataset[\"Cabin\"],order=['A','B','C','D','E','F','G','T','X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(y=\"Survived\",x=\"Cabin\",data=dataset,kind=\"bar\",order=['A','B','C','D','E','F','G','T','X'])\n",
    "g = g.set_ylabels(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Ticket\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n",
    "\n",
    "Ticket = []\n",
    "for i in list(dataset.Ticket):\n",
    "    if not i.isdigit() :\n",
    "        Ticket.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0]) #Take prefix\n",
    "    else:\n",
    "        Ticket.append(\"X\")\n",
    "        \n",
    "dataset[\"Ticket\"] = Ticket\n",
    "dataset[\"Ticket\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical values for Pclass\n",
    "dataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\n",
    "dataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless variables \n",
    "dataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[:train_len]\n",
    "test = dataset[train_len:]\n",
    "test.drop(labels=[\"Survived\"],axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Survived\"] = train[\"Survived\"].astype(int)\n",
    "\n",
    "Y_train = train[\"Survived\"]\n",
    "\n",
    "X_train = train.drop(labels = [\"Survived\"],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Simple modeling\n",
    "#### 6.1.1 Cross validate models\n",
    "I compared 10 popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure.\n",
    "\n",
    ". SVC\n",
    "\n",
    ". Decision Tree\n",
    "\n",
    ". AdaBoost\n",
    "\n",
    ". Random Forest\n",
    "\n",
    ". Extra Trees\n",
    "\n",
    ". Gradient Boosting\n",
    "\n",
    ". Multiple layer perceprton (neural network)\n",
    "\n",
    ". KNN\n",
    "\n",
    ". Logistic regression\n",
    "\n",
    ". Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate model with Kfold stratified cross val\n",
    "kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Modeling step Test differents algorithms \n",
    "random_state = 2\n",
    "classifiers = []\n",
    "classifiers.append(SVC(random_state=random_state))\n",
    "classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n",
    "classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
    "classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
    "classifiers.append(MLPClassifier(random_state=random_state))\n",
    "classifiers.append(KNeighborsClassifier())\n",
    "classifiers.append(LogisticRegression(random_state = random_state))\n",
    "classifiers.append(LinearDiscriminantAnalysis())\n",
    "\n",
    "cv_results = []\n",
    "for classifier in classifiers :\n",
    "    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n",
    "\n",
    "cv_means = []\n",
    "cv_std = []\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "\n",
    "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n",
    "\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n",
    "\n",
    "g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n",
    "g.set_xlabel(\"Mean Accuracy\")\n",
    "g = g.set_title(\"Cross validation scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### META MODELING  WITH ADABOOST, RF, EXTRATREES and GRADIENTBOOSTING\n",
    "\n",
    "# Adaboost\n",
    "DTC = DecisionTreeClassifier()\n",
    "\n",
    "adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
    "\n",
    "ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "              \"n_estimators\" :[1,2],\n",
    "              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n",
    "\n",
    "gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n",
    "\n",
    "gsadaDTC.fit(X_train,Y_train)\n",
    "\n",
    "ada_best = gsadaDTC.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsadaDTC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExtraTrees \n",
    "ExtC = ExtraTreesClassifier()\n",
    "\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "ex_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "gsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n",
    "\n",
    "gsExtC.fit(X_train,Y_train)\n",
    "\n",
    "ExtC_best = gsExtC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsExtC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFC Parameters tunning \n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "rf_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n",
    "\n",
    "gsRFC.fit(X_train,Y_train)\n",
    "\n",
    "RFC_best = gsRFC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsRFC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting tunning\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "gb_param_grid = {'loss' : [\"deviance\"],\n",
    "              'n_estimators' : [100,200,300],\n",
    "              'learning_rate': [0.1, 0.05, 0.01],\n",
    "              'max_depth': [4, 8],\n",
    "              'min_samples_leaf': [100,150],\n",
    "              'max_features': [0.3, 0.1] \n",
    "              }\n",
    "\n",
    "gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n",
    "\n",
    "gsGBC.fit(X_train,Y_train)\n",
    "\n",
    "GBC_best = gsGBC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SVC classifier\n",
    "SVMC = SVC(probability=True)\n",
    "svc_param_grid = {'kernel': ['rbf'], \n",
    "                  'gamma': [ 0.001, 0.01, 0.1, 1],\n",
    "                  'C': [1, 10, 50, 100,200,300, 1000]}\n",
    "\n",
    "gsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n",
    "\n",
    "gsSVMC.fit(X_train,Y_train)\n",
    "\n",
    "SVMC_best = gsSVMC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsSVMC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Plot learning curves\n",
    "Learning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "g = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,Y_train,cv=kfold)\n",
    "g = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\n",
    "g = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,Y_train,cv=kfold)\n",
    "g = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,Y_train,cv=kfold)\n",
    "g = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,Y_train,cv=kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoosting and Adaboost classifiers tend to overfit the training set. According to the growing cross-validation curves GradientBoosting and Adaboost could perform better with more training examples.\n",
    "\n",
    "SVC and ExtraTrees classifiers seem to better generalize the prediction since the training and cross-validation curves are close together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 Feature importance of tree based classifiers\n",
    "In order to see the most informative features for the prediction of passengers survival, i displayed the feature importance for the 4 tree based classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = ncols = 2\n",
    "fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n",
    "\n",
    "names_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n",
    "\n",
    "nclassifier = 0\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        name = names_classifiers[nclassifier][0]\n",
    "        classifier = names_classifiers[nclassifier][1]\n",
    "        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n",
    "        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n",
    "        g.set_xlabel(\"Relative importance\",fontsize=12)\n",
    "        g.set_ylabel(\"Features\",fontsize=12)\n",
    "        g.tick_params(labelsize=9)\n",
    "        g.set_title(name + \" feature importance\")\n",
    "        nclassifier += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\n",
    "test_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\n",
    "test_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\n",
    "test_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\n",
    "test_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n",
    "\n",
    "\n",
    "# Concatenate all classifier results\n",
    "ensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n",
    "\n",
    "\n",
    "g= sns.heatmap(ensemble_results.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The prediction seems to be quite similar for the 5 classifiers except when Adaboost is compared to the others classifiers.\n",
    "\n",
    "#The 5 classifiers give more or less the same prediction but there is some differences. Theses differences between the 5 classifier predictions are sufficient to consider an ensembling vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Ensemble modeling\n",
    "#### 6.2.1 Combining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n",
    "('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n",
    "\n",
    "votingC = votingC.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.3 Prediction\n",
    "#### 6.3.1 Predict and Submit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n",
    "\n",
    "results = pd.concat([IDtest,test_Survived],axis=1)\n",
    "\n",
    "results.to_csv('C:\\\\Users\\\\User\\\\data\\\\titanic\\\\submission_ensemble.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
